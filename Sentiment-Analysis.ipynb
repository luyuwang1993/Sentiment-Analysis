{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy, re\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import string\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize,RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords, wordnet, sentiwordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import emoji\n",
    "    \n",
    "# import metrics to show accuracy, recall, precision and ...\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon') # vader sentiment\n",
    "nltk.download('sentiwordnet') # sentiwordnet sentiment\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data overview\n",
    "def df_overview(df):\n",
    "    print ('Rows     : ', df.shape[0])\n",
    "    print ('Columns  : ', df.shape[1])\n",
    "    print ('\\nFeatures : ', df.columns.tolist())\n",
    "    print ('\\nMissing values :  ', df.isnull().sum().values.sum())\n",
    "    print ('\\nUnique values :  \\n', df.nunique())\n",
    "    return df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset and remove empty rows\n",
    "raw_df = pd.read_csv('../datastore/Manuel_Antonio_National_Park/review-info.dat', sep='\\t', lineterminator='\\n')\n",
    "df = raw_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['reviewAuthor', 'visitDate', 'reviewDate', 'reviewStars', 'reviewTotalReviews', 'reviewAuthorAddress', 'reviewTitle', 'translated_reviewText']]\n",
    "df.columns = ['author', 'exp_date', 'review_date', 'rating', 'review_num', 'address', 'title', 'review']\n",
    "curr_rows = df_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset having author's geo infomation, and join it with review data\n",
    "loc_df = pd.read_csv('../datastore/Manuel_Antonio_National_Park/review-info-with-loc.dat', sep='\\t')\n",
    "loc_df = loc_df[['reviewAuthor', 'reviewAuthorLng', 'reviewAuthorLng', 'reviewAuthorCountryCode', 'reviewAuthorCountryName']]\n",
    "loc_df.columns = ['author', 'lat', 'lng', 'country_code', 'country_name']\n",
    "loc_df.head(5)\n",
    "print(\"# of Row: {}\".format(loc_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data \n",
    "# remove duplicates\n",
    "loc_df = loc_df.drop_duplicates(subset=['author'], keep='last')\n",
    "print(\"# of Row: {}\".format(loc_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data \n",
    "# remove duplicates\n",
    "df = df.drop_duplicates(subset=['author', 'review'], keep='last')\n",
    "print(\"\\n\")\n",
    "print(\"\\n======================\")\n",
    "print(\"Cleaned {} duplicate rows!\".format(curr_rows - df_overview(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join tables\n",
    "df = df.merge(loc_df, on='author', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
    "def get_word_sentiment(word, tag):\n",
    "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
    "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
    "    #Some of the words have only one Synset and some have several.\n",
    "    synsets = wordnet.synsets(word, pos=tag)\n",
    "    \n",
    "    if not synsets:\n",
    "        return ['', 0, 0, 0]\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = sentiwordnet.senti_synset(synset.name())\n",
    "\n",
    "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    pos_tags = pos_tag(text.split())\n",
    "    pos_score, neg_score, obj_score, word_count = 0, 0, 0, 0\n",
    "    for word, tag in pos_tags:\n",
    "        res = get_word_sentiment(word, get_wordnet_pos(tag))\n",
    "        pos_score += res[1]\n",
    "        neg_score += res[2]\n",
    "        obj_score += res[3]\n",
    "        word_count += 1\n",
    "        \n",
    "    return [pos_score/word_count, neg_score/word_count, obj_score/word_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lower the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove hyperlink\n",
    "    text = re.sub(r'http.?://[^\\s]+[\\s]?', '', text)\n",
    "    \n",
    "    # ad-hoc remove\n",
    "    text = text.replace(\"cr \", \"costa rica \").replace(\"CR \", \"costa rica\")\\\n",
    "    .replace(\"manual \", \"manuel \").replace(\"Manual \", \"manuel \")\\\n",
    "    .replace(\"manuel antonio\", \"\").replace(\"esp \", \"especially \").replace(\"parc \", \"park \")\\\n",
    "    .replace(\"&quot;\", \"\").replace(\"&#39;\", \"'\").replace(\"；\", \";\")\\\n",
    "    .replace(\"。\", \".\").replace(\"，\", \",\").replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"`\", \"'\")\n",
    "    \n",
    "    # spelling check\n",
    "    from textblob import TextBlob\n",
    "    text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # converting emoji\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r':[a-z_&]+:', '', text)\n",
    "\n",
    "    # replace all types of negations: no, n't, never\n",
    "    text = text.replace(\"he's\", \"he is\").replace(\"I'm\", \"I am\").replace(\"'re\", \" are\").replace(\"ain't\", \"are not\")\\\n",
    "    .replace(\"'ve\", \" have\").replace(\"'ll\", \" will\").replace(\"won't\", \"will not\").replace(\"can't\", \"can not\")\\\n",
    "    .replace(\"n't\", \" not\").replace(\"'d\", \" would\")\n",
    "    \n",
    "    # remove all symbols\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # tokenize the text with removal of non-words,  punctuation, short (< 3 symbols) and long (> 25 symbols) tokens\n",
    "    text = [word.strip() for word in text.split() if len(word.strip()) >= 3 and len(word.strip()) <= 25]\n",
    "\n",
    "    # filtering English stopwords, and remove digits\n",
    "    stop = stopwords.words('english')\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    text = [word for word in text if word not in stop]\n",
    "\n",
    "    # Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database (retaining nouns and adjectives)\n",
    "    pos_tags = pos_tag(text)\n",
    "    pos_tags = list(filter(lambda x: get_wordnet_pos(x[1]) is not None, pos_tags))\n",
    "    text = [w for w, t in pos_tags if get_wordnet_pos(t) == wordnet.NOUN or get_wordnet_pos(t) == wordnet.ADJ]\n",
    "\n",
    "    # stemming (reducing inflected words to their word stems using Porter stemmer)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "\n",
    "    # lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)\n",
    "    text = [WordNetLemmatizer().lemmatize(w, get_wordnet_pos(t)) for w, t in pos_tags]\n",
    "    \n",
    "    # join the token\n",
    "    text = ' '.join(text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review_clean\"] = df[\"review\"].apply(lambda x: clean_text(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (VADER) add sentiment anaylsis columns\n",
    "df[\"sentiments\"] = df[\"review_clean\"].apply(lambda x: sentiment_analyzer.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiwordnet sentiment\n",
    "df[\"sentiments\"] = df[\"review_clean\"].apply(lambda x: get_sentiment(x))\n",
    "df = pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)\n",
    "df.columns = ['author', 'exp_date', 'review_date', 'rating', 'review_num', 'address', 'title', 'raw_review', 'lat', 'lng', 'country_code', 'country_name', 'cleaned_review', 'vader_neg', 'vader_neu', 'vader_pos', 'vader_compound', 'swn_pos', 'swn_neg', 'swn_obj']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index += 1\n",
    "df.to_csv('sentiment-manual-antonio.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2700 rows of data in total,\n",
    "# label first ~10% data in sentiment-manual-antonio.csv by review rating. \n",
    "# label -1 if review rating below and equal to 3; label 1 if review rating over and equal to 4.\n",
    "# so we will have ~90% data left for training and validation\n",
    "TRAIN_AND_VALIDATION_END_INDEX = 2700\n",
    "TEST_SET_END_INDEX = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"label-sentiment-manual-antonio.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test_labels = []\n",
    "test_reviews = []\n",
    "\n",
    "vader_tp = 0\n",
    "vader_fp = 0\n",
    "vader_tn = 0\n",
    "vader_fn = 0\n",
    "vader_pos = 0\n",
    "vader_neg = 0\n",
    "vader_neu = 0\n",
    "\n",
    "wn_tp = 0\n",
    "wn_fp = 0\n",
    "wn_tn = 0\n",
    "wn_fn = 0\n",
    "wn_pos = 0\n",
    "wn_neg = 0\n",
    "wn_neu = 0\n",
    "\n",
    "num_of_test = 0\n",
    "cnt_neutral = 0\n",
    "for index in range(0, TEST_SET_END_INDEX):\n",
    "    vader_score = float(df['vader_compound'][index])\n",
    "    wn_score = float(df['swn_pos'][index])-float(df['swn_neg'][index])\n",
    "    label_val = int(df['label'][index])\n",
    "    \n",
    "    num_of_test += 1\n",
    "    \n",
    "    test_labels.append(df['label'][index])\n",
    "    test_reviews.append(df['cleaned_review'][index])\n",
    "    \n",
    "    if (vader_score < 0.05 and vader_score > -0.05) or wn_score == 0:\n",
    "        cnt_neutral += 1\n",
    "        \n",
    "    else:        \n",
    "        vader_final = 1 if vader_score >= 0.05 else -1\n",
    "        wn_final = 1 if wn_score > 0 else -1\n",
    "\n",
    "        vader_tp += 1 if vader_final == label_val and label_val == 1 else 0\n",
    "        vader_tn += 1 if vader_final == label_val and label_val == -1 else 0\n",
    "        vader_fn += 1 if vader_final != label_val and label_val == 1 else 0\n",
    "        vader_fp += 1 if vader_final != label_val and label_val == -1 else 0\n",
    "\n",
    "        wn_tp += 1 if wn_final == label_val and label_val == 1 else 0\n",
    "        wn_tn += 1 if wn_final == label_val and label_val == -1 else 0\n",
    "        wn_fn += 1 if wn_final != label_val and label_val == 1 else 0\n",
    "        wn_fp += 1 if wn_final != label_val and label_val == -1 else 0\n",
    "        \n",
    "    if vader_score < 0.05 and vader_score > -0.05:\n",
    "        vader_neu += 1\n",
    "    elif vader_score >= 0.05:\n",
    "        vader_pos += 1\n",
    "    else:\n",
    "        vader_neg += 1\n",
    "            \n",
    "    if wn_score == 0:\n",
    "        wn_neu += 1\n",
    "    elif wn_score >0:\n",
    "        wn_pos += 1\n",
    "    else:\n",
    "        wn_neg += 1\n",
    "\n",
    "vader_accuracy_score = (vader_tp + vader_fn)/(num_of_test - cnt_neutral)\n",
    "vader_precision_score = vader_tp/(vader_tp + vader_fp)\n",
    "vader_recall_score = vader_tp/(vader_tp + vader_fn)\n",
    "vader_f1_score = 2 * (vader_precision_score * vader_recall_score) / (vader_precision_score + vader_recall_score)\n",
    "\n",
    "print(\"=== Vader ===\\n\")\n",
    "print(\"TruePositive: %d\" % vader_tp)\n",
    "print(\"TrueNegative: %d\" % vader_tn)\n",
    "print(\"FalsePositive: %d\" % vader_fp)\n",
    "print(\"FalseNegative: %d\\n\" % vader_fn)\n",
    "print(\"Accuracy Score: %f\" % vader_accuracy_score)\n",
    "print(\"Precision Score: %f\" % vader_precision_score)\n",
    "print(\"Recall Score: %f\" % vader_recall_score)\n",
    "print(\"F1 Score: %f\\n\" % vader_f1_score)\n",
    "# in 300\n",
    "print(\"Positive: %d\" % vader_pos)\n",
    "print(\"Negative: %d\" % vader_neg)\n",
    "print(\"Neutral: %d\\n\" % vader_neu)\n",
    "\n",
    "wn_accuracy_score = (wn_tp + wn_fn)/(num_of_test - cnt_neutral)\n",
    "wn_precision_score = wn_tp/(wn_tp + wn_fp)\n",
    "wn_recall_score = wn_tp/(wn_tp + wn_fn)\n",
    "wn_f1_score = 2 * (wn_precision_score * wn_recall_score) / (wn_precision_score + wn_recall_score)\n",
    "\n",
    "print(\"=== Sentiment WordNet ===\\n\")\n",
    "print(\"TruePositive: %d\" % wn_tp)\n",
    "print(\"TrueNegative: %d\" % wn_tn)\n",
    "print(\"FalsePositive: %d\" % wn_fp)\n",
    "print(\"FalseNegative: %d\\n\" % wn_fn)\n",
    "print(\"Accuracy Score: %f\" % wn_accuracy_score)\n",
    "print(\"Precision Score: %f\" % wn_precision_score)\n",
    "print(\"Recall Score: %f\" % wn_recall_score)\n",
    "print(\"F1 Score: %f\\n\" % wn_f1_score)\n",
    "# in 300\n",
    "print(\"Positive: %d\" % wn_pos)\n",
    "print(\"Negative: %d\" % wn_neg)\n",
    "print(\"Neutral: %d\" % wn_neu)\n",
    "\n",
    "print(\"Removed neutral [%d] reviews\" % cnt_neutral)\n",
    "print(\"# of Test -> %d\" % num_of_test)\n",
    "test_reviews_tokens = [r for r in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation set\n",
    "labels = []\n",
    "reviews = []\n",
    "\n",
    "num_of_train_and_validation = 0\n",
    "for index in range(TEST_SET_END_INDEX, TRAIN_AND_VALIDATION_END_INDEX):\n",
    "    labels.append(df['label'][index])\n",
    "    reviews.append(df['cleaned_review'][index])\n",
    "    \n",
    "    num_of_train_and_validation += 1\n",
    "\n",
    "print(\"# of Train and Validation -> %d\" % num_of_train_and_validation)\n",
    "reviews_tokens = [r for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual split data into training set and test set\n",
    "x_train = reviews_tokens\n",
    "x_test = test_reviews_tokens\n",
    "y_train = labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding label using LabelEncoder()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)\n",
    "\n",
    "MIN_FEATURE = 100 # inclusive\n",
    "MAX_FEATURE = 7100 # exclusive\n",
    "FEATURE_INTERVAL = 100\n",
    "\n",
    "# load the module to transform our review inputs into word vectors using TfidVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "training_methods = []\n",
    "\n",
    "# metrics \n",
    "metrics = {\"performance\": [], \"accuracy\": [], \"cross_val\": [], \"f1\": [], \"precision\": [], \"recall\": []}\n",
    "\n",
    "# create our SVM classifier with the class LinearSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "training_methods.append((LinearSVC(), \"linear svc\", copy.deepcopy(metrics)))\n",
    "\n",
    "# create our Bayes classifier with the BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "training_methods.append((BernoulliNB(binarize=None), \"bayes with bernoulliNB\", copy.deepcopy(metrics)))\n",
    "\n",
    "# create our Bayes classifier with the MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "training_methods.append((MultinomialNB(), \"bayes with multinomialNB\", copy.deepcopy(metrics)))\n",
    "\n",
    "# x_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "for n_features in range(MIN_FEATURE, MAX_FEATURE, FEATURE_INTERVAL):\n",
    "    # print(\"------------------------------------\\n\")\n",
    "    # print(\"nFeatures: %s\\n\" % n_features)\n",
    "    tfidf_vect = TfidfVectorizer(max_features=n_features)\n",
    "    # tfidf_vect.fit(reviews_tokens)\n",
    "\n",
    "    _x_train = tfidf_vect.fit_transform(x_train)\n",
    "    _x_test = tfidf_vect.transform(x_test)\n",
    "     \n",
    "    # print(tfidf_vect.vocabulary_)\n",
    "    # print(x_train)\n",
    "    \n",
    "    for tm, name, metrics in training_methods:\n",
    "        tm.fit(_x_train, y_train)\n",
    "        \n",
    "        prediction = tm.predict(_x_test)\n",
    "        \n",
    "        if n_features == 500:\n",
    "            print(\"Model [%s] at [%d] features ->\\n\" % (name, n_features))\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()\n",
    "            print(\"TruePositive: %d\" % tp)\n",
    "            print(\"TrueNegative: %d\" % tn)\n",
    "            print(\"FalsePositive: %d\" % fp)\n",
    "            print(\"FalseNegative: %d\\n\" % fn)\n",
    "        \n",
    "        _performance_score = tm.score(_x_test, y_test)\n",
    "        _accuracy_score = accuracy_score(y_test, prediction)\n",
    "        _f1_score = f1_score(y_test, prediction)\n",
    "        _precision_score = precision_score(y_test, prediction)    \n",
    "        _recall_score = recall_score(y_test, prediction)\n",
    "        \n",
    "        metrics[\"performance\"].append((n_features, _performance_score))\n",
    "        metrics[\"accuracy\"].append((n_features, _accuracy_score))\n",
    "        metrics[\"f1\"].append((n_features, _f1_score))\n",
    "        metrics[\"precision\"].append((n_features, _precision_score))\n",
    "        metrics[\"recall\"].append((n_features, _recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the diagram\n",
    "nfeatures_performance = []\n",
    "nfeatures_accuracy = []\n",
    "nfeatures_f1 = []\n",
    "nfeatures_precision = []\n",
    "nfeatures_recall = []\n",
    "for tm, name, metrics in training_methods:\n",
    "    nfeatures_performance.append((name, pd.DataFrame(metrics['performance'], columns=['n_features', 'performance_score'])))\n",
    "    nfeatures_accuracy.append((name, pd.DataFrame(metrics['accuracy'], columns=['n_features', 'accuracy_score'])))\n",
    "    nfeatures_f1.append((name, pd.DataFrame(metrics['f1'], columns=['n_features', 'f1_score'])))\n",
    "    nfeatures_precision.append((name, pd.DataFrame(metrics['precision'], columns=['n_features', 'precision_score'])))\n",
    "    nfeatures_recall.append((name, pd.DataFrame(metrics['recall'], columns=['n_features', 'recall_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "title = \"Performance Score Comparison between\"\n",
    "\n",
    "for name, df in nfeatures_performance:\n",
    "    plt.plot(df.n_features, df.performance_score, label=name)\n",
    "    title += \" %s |\" % name\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Performance Score\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"=== Performance Score ===\\n\")\n",
    "for metrics in nfeatures_performance:\n",
    "    max_performance_score_feature = 0\n",
    "    max_performance_score = 0\n",
    "    \n",
    "    for feature_index in range(0, int((MAX_FEATURE-MIN_FEATURE)/FEATURE_INTERVAL)):\n",
    "        _nfeatures = metrics[1].loc[feature_index].n_features\n",
    "        _performance_score = metrics[1].loc[feature_index].performance_score\n",
    "        # print(\"[%d] features -> [%f] performance score\" % (_nfeatures, _performance_score))\n",
    "        if _performance_score > max_performance_score:\n",
    "            max_performance_score = _performance_score\n",
    "            max_performance_score_feature = _nfeatures\n",
    "    \n",
    "    print(\"Model Name [%s] has max performance score [%f] at [%d] # of features\" % (metrics[0], max_performance_score, max_performance_score_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "title = \"Accuracy Score Comparison between\"\n",
    "\n",
    "for name, df in nfeatures_accuracy:\n",
    "    plt.plot(df.n_features, df.accuracy_score, label=name)\n",
    "    title += \" %s |\" % name\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"=== Accuracy Score ===\\n\")\n",
    "for metrics in nfeatures_accuracy:\n",
    "    max_accuracy_score_feature = 0\n",
    "    max_accuracy_score = 0\n",
    "    \n",
    "    for feature_index in range(0, int((MAX_FEATURE-MIN_FEATURE)/FEATURE_INTERVAL)):\n",
    "        _nfeatures = metrics[1].loc[feature_index].n_features\n",
    "        _accuracy_score = metrics[1].loc[feature_index].accuracy_score\n",
    "        # print(\"[%d] features -> [%f] accuracy score\" % (_nfeatures, _accuracy_score))\n",
    "        if _accuracy_score > max_accuracy_score:\n",
    "            max_accuracy_score = _accuracy_score\n",
    "            max_accuracy_score_feature = _nfeatures\n",
    "    \n",
    "    print(\"Model Name [%s] has max accuracy score [%f] at [%d] # of features\" % (metrics[0], max_accuracy_score, max_accuracy_score_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "title = \"F1 Score Comparison between\"\n",
    "\n",
    "for name, df in nfeatures_f1:\n",
    "    plt.plot(df.n_features, df.f1_score, label=name)\n",
    "    title += \" %s |\" % name\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"=== F1 Score ===\\n\")\n",
    "for metrics in nfeatures_f1:\n",
    "    max_f1_score_feature = 0\n",
    "    max_f1_score = 0\n",
    "    \n",
    "    for feature_index in range(0, int((MAX_FEATURE-MIN_FEATURE)/FEATURE_INTERVAL)):\n",
    "        _nfeatures = metrics[1].loc[feature_index].n_features\n",
    "        _f1_score = metrics[1].loc[feature_index].f1_score\n",
    "        # print(\"[%d] features -> [%f] F1 score\" % (_nfeatures, _f1_score))\n",
    "        if _f1_score > max_f1_score:\n",
    "            max_f1_score = _f1_score\n",
    "            max_f1_score_feature = _nfeatures\n",
    "    \n",
    "    print(\"Model Name [%s] has max F1 score [%f] at [%d] # of features\" % (metrics[0], max_f1_score, max_f1_score_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "title = \"Precision Score Comparison between\"\n",
    "\n",
    "for name, df in nfeatures_precision:\n",
    "    plt.plot(df.n_features, df.precision_score, label=name)\n",
    "    title += \" %s |\" % name\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Precision Score\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"=== Precision Score ===\\n\")\n",
    "for metrics in nfeatures_precision:\n",
    "    max_precision_score_feature = 0\n",
    "    max_precision_score = 0\n",
    "    \n",
    "    for feature_index in range(0, int((MAX_FEATURE-MIN_FEATURE)/FEATURE_INTERVAL)):\n",
    "        _nfeatures = metrics[1].loc[feature_index].n_features\n",
    "        _precision_score = metrics[1].loc[feature_index].precision_score\n",
    "        # print(\"[%d] features -> [%f] Precision score\" % (_nfeatures, _precision_score))\n",
    "        if _precision_score > max_precision_score:\n",
    "            max_precision_score = _precision_score\n",
    "            max_precision_score_feature = _nfeatures\n",
    "    \n",
    "    print(\"Model Name [%s] has max Precision score [%f] at [%d] # of features\" % (metrics[0], max_precision_score, max_precision_score_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "title = \"Recall Score Comparison between\"\n",
    "\n",
    "for name, df in nfeatures_recall:\n",
    "    plt.plot(df.n_features, df.recall_score, label=name)\n",
    "    title += \" %s |\" % name\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"=== Recall Score ===\\n\")\n",
    "for metrics in nfeatures_recall:\n",
    "    max_recall_score_feature = 0\n",
    "    max_recall_score = 0\n",
    "    \n",
    "    for feature_index in range(0, int((MAX_FEATURE-MIN_FEATURE)/FEATURE_INTERVAL)):\n",
    "        _nfeatures = metrics[1].loc[feature_index].n_features\n",
    "        _recall_score = metrics[1].loc[feature_index].recall_score\n",
    "        # print(\"[%d] features -> [%f] Recall score\" % (_nfeatures, _recall_score))\n",
    "        if _recall_score > max_recall_score:\n",
    "            max_recall_score = _recall_score\n",
    "            max_recall_score_feature = _nfeatures\n",
    "    \n",
    "    print(\"Model Name [%s] has max Recall score [%f] at [%d] # of features\" % (metrics[0], max_recall_score, max_recall_score_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, f1, accuracy for all methods\n",
    "\n",
    "lsvc_accuracy_score = nfeatures_accuracy[0][1].loc[4].accuracy_score\n",
    "lsvc_f1_score = nfeatures_f1[0][1].loc[4].f1_score\n",
    "lsvc_precision_score = nfeatures_precision[0][1].loc[4].precision_score\n",
    "lsvc_recall_score = nfeatures_recall[0][1].loc[4].recall_score\n",
    "\n",
    "bcbn_accuracy_score = nfeatures_accuracy[1][1].loc[4].accuracy_score\n",
    "bcbn_f1_score = nfeatures_f1[1][1].loc[4].f1_score\n",
    "bcbn_precision_score = nfeatures_precision[1][1].loc[4].precision_score\n",
    "bcbn_recall_score = nfeatures_recall[1][1].loc[4].recall_score\n",
    "\n",
    "bcmn_accuracy_score = nfeatures_accuracy[2][1].loc[4].accuracy_score\n",
    "bcmn_f1_score = nfeatures_f1[2][1].loc[4].f1_score\n",
    "bcmn_precision_score = nfeatures_precision[2][1].loc[4].precision_score\n",
    "bcmn_recall_score = nfeatures_recall[2][1].loc[4].recall_score\n",
    "\n",
    "val1 = [\"Accuracy_Score\", \"F1_Score\", \"Precision_Score\", \"Recall_Score\"] \n",
    "val2 = [\"Vader\", \"Sentiment_WordNet\", \"Linear_SVC\", \"Naive_Bayes_with_BernoulliNB\", \"Naive_Bayes_with_MultinomialNB\"] \n",
    "val3 = [[vader_accuracy_score, vader_f1_score, vader_precision_score, vader_recall_score],\n",
    "        [wn_accuracy_score, wn_f1_score, wn_precision_score, wn_recall_score],\n",
    "        [lsvc_accuracy_score, lsvc_f1_score, lsvc_precision_score, lsvc_recall_score],\n",
    "        [bcbn_accuracy_score, bcbn_f1_score, bcbn_precision_score, bcbn_recall_score],\n",
    "        [bcmn_accuracy_score, bcmn_f1_score, bcmn_precision_score, bcmn_recall_score]]\n",
    "  \n",
    "fig, ax = plt.subplots() \n",
    "ax.set_axis_off() \n",
    "table = ax.table( \n",
    "    cellText = val3,  \n",
    "    rowLabels = val2,  \n",
    "    colLabels = val1, \n",
    "    rowColours =[\"palegreen\"] * 10,  \n",
    "    colColours =[\"palegreen\"] * 10, \n",
    "    cellLoc ='center',  \n",
    "    loc ='upper left')  \n",
    "\n",
    "\n",
    "# table = ax.table(cellText=table_data, loc='center')\n",
    "table.set_fontsize(14)\n",
    "table.scale(3.5,3.5)\n",
    "# ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
